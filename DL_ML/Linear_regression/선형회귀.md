## 선형회귀

- 선형회귀란 여러 형태와 위치에 분포해 있는 데이터들을 가장 잘 대변해주는 선을 찾아내는 것이다. 데이터에 가장 잘 맞는 하나의 선(최적선)을 찾아내는 것이다.

### 선형회귀 용어

- 지도학습, 비지도학습, 강화학습

- *선형회귀*는 *지도학습* 알고리즘에 해당한다. 또한, 연속적인 값을 예측하므로 *회귀*에 해당한다. 

- 선형회귀에서 우리가 맞추려고 하는 값은 *목표변수*(target variable/output)에 해당한다. 그리고, 우리가 그 목표변수를 맞추기 위해서 사용하는 값을 *입력변수*(input variable/feature)라고 한다. 입력변수(특징)을 이용해 목표변수를 예측하는 방식이다.

- 학습데이터의 개수를 m이라고 표현하고, 예를 들어, m=50이라고 한다면 50개의 데이터를 갖고 프로그램을 학습시키는 것이다.

- 입력변수는 x라고 표현하고, 목표변수는 y라고 표현한다. 

#### 가설함수
- 우리는 최적선을 찾아내기 위해 다양한 함수를 시도해보는데, 이 함수 하나하나들을 가설함수(hypothesis function)이라고 부른다. 일단 선형회귀에서 우리가 찾으려는 선은 곡선이 아닌 직선이다. y=ax+b의 형태인데, 우리는 최적의 a와 b를 찾아내는 것이다. 가설함수 h(x)를 표현할때 y=ax+b 대신, 우리는 h(x)=θ_0(세타_0) + θ_1x 와 같이 표현한다. 변수가 늘어나 y가 생길경우, y의 계수를 θ_2 이런식으로 늘리는 방식이다. ex. h_θ(x) = θ_0 + θ_1x + θ_2y 이런식으로 말이다.θ

#### 평균 제곱 오차(MSE)
- 입력변수가 딱 하나밖에 없다고 가정하고, 집 크기(입력변수) 를 토대로 집 가격(목표변수)를 예측하는 가설함수는 *h_θ(x)=θ_0+θ_1x* 이렇게 생겼다. 그리고 세 개의 가설함수가 있다고 가정했을때, 조금 더 잘 맞는 가설함수를 찾기 위해서 어떤 기준을 두고 평가한다. 가설함수를 평가하는 방법중 하나인 *평균 제곱 오차법* 은 데이터들과 가설함수가 평균적으로 얼마나 떨어져있는지를 나타내는 방식이다. 각각의 데이터들이 가설함수를 통해 예측한 값과 실제 값의 차이들을 제곱한 값들을 모두 더해준뒤, 총 데이터 개수 만큼 나눈 것이 평균 제곱 오차이다. 이 값이 크게 나온다는 것은 결국 가설함수가 예측을 성공적으로 하지 못한다는 것이다. 오차에 제곱을 하는 이유는 모든 오차를 양수로 통일 할수 있다는 것이고, 또한 더 큰 오차를 부각하기 위함이다. 

#### 손실함수 (loss function, cost function)
- 우리가 만든 가설함수가 얼마나 최적선인지를 평가할 기준이 필요하고, 선형회귀에서는 일반적으로 평균제곱오차(MSE) 라는 것을 통해서 평가한다. 평균제곱오차가 크면 가설함수가 데이터에 잘 안맞는다는 것이고, 평균제곱오차가 작으면 가설함수가 데이터에 잘 맞는다는 것이다. *손실함수*는 가설함수를 평가하는 함수이다.