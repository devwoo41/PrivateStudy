#### Feature Scaling: Normalization
- 데이터 전처리를 하는 이유는 주어진 데이터를 그대로 사용하지 않고, 각오해서 모델을 학습시키는데 좀 더 좋은 형식으로 만들어줘야 하기 때문이다. 그 중에서 Feature Scaling을 먼저 볼건데, Feature Scaling은 Feature, 즉 입력변수의 크기를 Scaling 조정해 준다는 뜻이다. 머신러닝 모델에 사용할 입력변수들의 크기를 조정해서 일정 범위 내에 떨어지도록 바꾸는 것이다. 예를 들어 연봉이라는 입력변수가 있고 나이라는 입력변수가 있다고 할때, 사람의 연봉은 보통 몇천만원인데, 사람의 나이는 몇십밖에 되지 않아 차이가 너무 많이 나서 feature scaling을 통해서 정규화시켜줘서 맞춰준다는 것이다. 

- Feature Scaling을 하면 경사하강법을 좀 더 빨리 할 수 있다. 

#### min-max normalization
- normalization의 의미는 숫자의 크기를 0과 1 사이로 만든다. 데이터의 최솟값과 최댓값을 이용해서 데이터의 크기를 0과 1사이로 바꿔주는데, 우선 한 열에서 가장 큰 속성값과 가장 작은 속성값을 찾아낸다. 그래서 정규화되어진 값은 이 공식을 따른다. *해당값-최솟값 / 최댓값 - 최솟값* 예를 들어서 키라는 속성을 가진 열에서 최댓값이 210이고, 최솟값이 140인 경우, 해당값은 0.57에 해당한다. *x_new = x_old - x_min / x_max - x_min*

#### Feature Scaling과 경사하강법
- Feature Scaling을 통해 데이터를 전처리한다면 동그란 형태의 그래프가 나오면서 경사하강법의 속도가 빨라진다.

#### One-hot Encoding
- 머신러닝 데이터에는 두 종류가 있다. 수치형 데이터와 범주형 데이터이다. 선형회귀를 예시로써 들자면 경사하강법을 하고 계산을 위해서는 수치형 데이터가 필요하다. 그렇다면 범주형 데이터의 경우 수치형 데이터로 변환해서 사용한다. 혈액형을 예씨로 들면 A형은 1, AB형은 2, B형은 3, O형은 4 이런식으로 말이다. 그러나 이런 경우의 문제는 혈액형에 원하지 않는 크고 작다라는 개념이 생긴다는 것이다. 그래서 one-hot-encoding을 사용해 각 속성을 새로운 열로 만들어준다. 

#### 편향(Bias)과 분산(Variance)
- 편향이 높은 머신 러닝 모델은 너무 간단해서 주어진 데이터의 관계를 잘 학습하지 못하고 편향이 낮은 모델은 주어진  데이터의 관계를 아주 잘 학습한다.
- 분산은 다양한 테스트 데이터가 주어졌을 때 모델의 성능이 얼마나 일관적으로 나오는지를 뜻한다.

#### 편향-분산 트레이드오프
- 선형 회귀 직선 형태: 복잡도가 떨어지기 때문에 곡선 관계를 학습할 수 없다. 어떤 데이터가 주어져도 일관적인 성능을 낸다. => 이런 경우 모델이 과소적합, 영어로는 underfit 되었다고 한다.
- 비선형 회귀의 곡선 형태: 트레이닝 데이터를 학습하는 게 아니라 외워버리는 것이다. 편향이 낮고 분산이 높은 모델이다. => 이런 경우 모델이 과적합, 영어로는 overfit 되었다고 한다.

- 일반적으로 편향과 분산은 하나가 줄어들수록 하나가 늘어나는 관계가 있다. 이러한 관계를 편향-분산 트레이드오프라고 한다. 이러한 것을 과소적합과 과적합의 밸런스를 찾고 있는 것이다.

#### 정규화
- 모델 과적합을 방지해주는 방법이다. 다항회귀를 통해 훈련데이터를 모두 통과하는 곡선을 그리기 위해서 곡선은 변화폭이 상당히 복잡하게 변한다. 근데 과적합이 일어나 편향이 낮고 분산이 큰 과적합이 일어나는 이유는 다항회귀에서 각각의 theta의 값들이 크기 때문에 일어난다. 그런데 정규화는 이 theta값들이 너무 커지는 것을 방지해 준다. theta값들이 너무 커지는 것을 방지해 준다면 트레이닝 데이터에 대한 오차는 조금 커질 수 있어도 위 아래로 변동이 엄청 심하던 가설함수를 좀 더 원만하게 만들어 줄 수 있고, 이는 여러 데이터셋에 일관된 성능을 만들어 줄 수 있다. 
- 이 정규화에는 L1 정규화, L2 정규화가 있다.