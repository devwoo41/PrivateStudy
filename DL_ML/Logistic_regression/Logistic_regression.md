### 지도학습에서의 분류문제
- 보통 분류문제에서는 각 결괏값에 어떤 숫자값을 지정해준다. 예로써, 스팸과 보통메일을 구별하는 경우, 보통 메일에는 0을 지정해주고 스팸메일에는 1을 지정해주고 이런식으로 말이다. 선형회귀는 예외적인 데이터에 너무 민감하게 반응해서 분류의 경우 잘 사용하지 않는다.

#### 로지스틱 회귀
- 앞서 말한 바와 같이 선형회귀는 예외적인 데이터에 너무 민감하게 반응해서, 분류의 경우에는 로지스틱 회귀를 사용한다. 로지스틱 회귀는 데이터에 가장 잘 맞는 일차함수가 아니라, 데이터에 가장 잘 맞는 시그모이드(sigmoid) 함수를 찾는다. 시그모이드 함수는 *S(x) = 1 / 1 + (e**(-x))* 와 같이 생겼다. 시그모이드 함수의 가장 중요한 특징은, 무조건 0과 1 사이의 결과만 나온다는 것이다. x에 양의 무한대가 갈 경우, 시그모이드 함수의 값은 1에 수렴하고, 음의 무한대에 갈 경우, 시그모이드 함수의 값은 0에 수렴한다. 로지스틱 회귀는 분류 문제인데, 로지스틱 회귀를 사용하는 이유는 시그모이드 함수의 결괏값은 결국 회귀에 근거하지만, 일반적으로 로지스틱 회귀를 분류에 사용할때는 시그모이드 함수의 값이 0.5를 넘냐 안넘냐를 기준으로 분류를 해주기 때문이다.

- x=[x_1,x_2,x_3,...,x_n], theta=[theta_1,theta_2,theta_3,...,theta_n] 

- g_theta(x) = theta.T * x

- h_theta(x) = 1 / 1 + (e**-(g_theta(x)))

#### 로그 손실
- 로지스틱 회귀의 손실함수는 *로그 손실(log-loss, cross-entropy)*이라는 걸 사용한다. 
- *logloss(h_theta(x),y) = -y*log(h_theta(x)) - (1-y)*log(1-h_theta(x))*

#### 분류 옵션이 3개 이상인 경우
- 각 옵션에 0,1,2 라는 숫자를 붙여준다. 처음에는 어떤 옵션이 A라는 옵션에 해당하는지 아닌지를 구한다. 이걸 h_theta(0)(x)라고 한다. 그리고 어떤 옵션이 B라는 옵션에 해당하는지 아닌지를 구하는 h_theta(1)(x) 라고 한다. 그리고 어떤 옵션이 C라는 옵션에 해당하는지 아닌지를 구하는 것을 h_theta(2)(X) 라고 한다. 이렇게 해서 만약 h_theta(0)(x)의 값이 0.6, h_theta(1)(x)의 값이 0.52, h_theta(2)(x)의 값이 0.78이 나온다면 이는 옵션 C에 해당한다는 의미이다.

#### 정확도, 정밀도, 재현율, 특이도
- 정확도(accuracy): (TP + TN) / (TP+TN+FP+FN) : 전체중에서 예측을 옳게 한(정확히 예측) 정도
- 정밀도(precision): TP / TP + FP : 맞다고 예측 + 실제로도 맞음 / 맞다고 예측하고 실제로도 맞은 경우 + 맞다고 예측했는데 실제로는 틀린 경우, 맞다고 예측했는데 실제로 맞은 정도
- 재현율(recall): TP / TP + FN : 실제로 positive인 데이터 중에서 positive로 예측된 비율
- 특이도(specificity): TN / TN + FP : 실제 negative인 데이터 중에서 negative로 예측한 비율