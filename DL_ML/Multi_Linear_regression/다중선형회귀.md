## 다중선형회귀
- 실제 머신러닝을 할때, 입력변수가 하나 뿐인 경우는 몹시 드물다. 보통은 훨씬 많은 속성, 입력변수를 가지고 예측을 하는데, 이런 경우 다중 선형회귀를 한다. 입력변수를 다른 말로 feature(속성)이라고 한다. 입력변수가 4개일 경우, n=4이고 입력변수는 x_1,x_2,x_3,x_4 이런식으로 표현이 된다. 그리고 똑같이 학습 데이터의 개수는 m이라고 표현한다. m=50이면 50개의 데이터를 가지고 학습을 한다. x(1) 이라고 하면 첫번째 데이터의 벡터 [x_1,x_2,x_3,x_4] 를 가리킨다. 즉 3번째 데이터의 2번째 열을 표현하고 싶다면. x(3)_2 이렇게 쓰면 되는것이다.

#### 다중 선형 회귀 가설 함수
- 다중 선형 회귀에서의 가설함수를 본다면, 우선 입력변수가 하나였을때를 복기시키자면. h(x)= theta_0 + theta_1 * x 이렇게 나타낼 수 있다. 선형회귀의 목적은 이 theta 값들을 조금씩 바꿔 주면서 최적값을 찾아내준다. 
- 입력변수가 여러개인 다중 선형회귀에서의 가설함수를 살펴보자. h_theta(x) = theta_0 + theta_1 * x_1 + theta_2 * x_2 + theta_3 * x_3 + ... + theta_n * x_n 이런식으로 나타낼 수 있다. 똑같이 목적은 theta 값들을 조금씩 조절하면서 학습데이터들에 맞는 theta값들을 찾는것이다. theta = [theta_0,theta_1,theta_2,theta_3], x = [1,x_1,x_2,x_3] 이렇게 표현하고 가설함수인 h(x) = theta.T * x 이렇게 표현하면 된다. 

#### 다중 선형 회귀 경사 하강법
- 손실함수라는 걸로 가설함수를 평가하는데, 손실함수가 크면 가설함수가 데이터에 잘 안맞는 다는 것이고, 손실함수가 적으면 가설함수가 데이터에 잘 맞다는 것이다. 경사하강법은 손실을 가장 빨리 줄이는 방향으로 theta값들을 바꿔주는 방법이다. 일반화하여 나타내면 경사하강공식은 n개의 입력변수를 상정한 경우, 
### *theta_j=theta_j-alpha*(1/m)[simga_1~m][{h_theta(x(i))-y(i)}*x_j[i]]* 와 같이 나타낼 수 있다.

#### 정규방정식
- 우리의 목적은 사실 극소점을 찾는것인데, 손실함수의 기울기가 0이 되는 지점을 찾으면 되는것이다. 이를 정규방정식(Normal Equation)이라고 한다.  입력 변수(속성)의 수가 엄청 많을 때는(1000개를 넘느냐를 기준으로 사용할 때가 많습니다) 경사 하강법을, 그리고 비교적 입력 변수의 수가 적을 때는 정규 방정식을 사용합니다.

#### convex 함수 - 아래로 볼록한 함수 ex. y= x**2
- convex 함수에서는 항상 경사 하강법이나 정규 방정식을 이용해서 최소점을 구할 수 있는 반면, 노트 위에서 봤던 non-convex 함수에서는 구한 극소점이 최소점이라고 확신할 수 없죠.